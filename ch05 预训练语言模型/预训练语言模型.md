### 5.1 理解注意力

#### 5.1.1 注意力介绍


用 $𝑿 = [𝒙_1, ⋯ , 𝒙_𝑁 ] ∈ ℝ_{𝐷×𝑁}$ 表示 $𝑁$ 组输入信息，其中 $𝐷$ 维向量 $𝒙_𝑛 ∈ℝ_𝐷$ , $𝑛 ∈ [1, 𝑁]$ 表示一组输入信息。为了节省计算资源，不需要将所有信息都输入神经网络，只需要从 $𝑿$ 中选择一些和任务相关的信息。注意力机制的计算可以分为两步：一是在所有输入信息上计算注意力分布，二是根据注意力分布来计算输入信息的加权平均。


为了从 $𝑁$ 个输入向量 $[𝒙_1, ⋯ , 𝒙_𝑁 ]$ 中选择出和某个特定任务相关的信息，需要引入一个和任务相关的表示，称为查询向量，并通过一个打分函数来计算每个输入向量和查询向量之间的相关性。给定一个和任务相关的查询向量 $𝒒$ ， 用注意力变量 $𝑧 ∈ [1, 𝑁]$ 来表示被选择信息的索引位置，即 $𝑧 = 𝑛$表示选择了第 $𝑛$ 个输入向量。为了方便计算，采用一种“软性”的信息选择机制．首先计算在给定 $𝒒$ 和 $𝑿$ 下，选择第 $𝑖$ 个输入向量的概率 $a$ :


$a_𝑛 = 𝑝(𝑧 = 𝑛|𝑿, 𝒒) = softmax (𝑠(𝒙_𝑛, 𝒒))=exp(s(x_n,q))/Σ_{j=1}^N exp(s(x_j,q))$


其中 $a_𝑛$ 称为注意力分布，𝑠(𝒙, 𝒒) 为注意力打分函数，
可以使用以下几种方式来计算：

- 加性模型 $𝑠(𝒙, 𝒒) = 𝒗^Ttanh(𝑾𝒙 + 𝑼𝒒)$
- 点积模型 $𝑠(𝒙, 𝒒) = 𝒙^Tq$
- 缩放点积模型 $𝑠(𝒙, 𝒒) = 𝒙^T𝒒/\sqrt𝐷$
- 双线性模型 $𝑠(𝒙, 𝒒) = 𝒙^T𝑾q$

其中 $𝑾, 𝑼, 𝒗$ 为可学习的参数，$𝐷$ 为输入向量的维度。


理论上，加性模型和点积模型的复杂度差不多，但是点积模型在实现上可以更好地利用矩阵乘积，从而计算效率更高。当输入向量的维度 $𝐷$ 比较高时，点积模型的值通常有比较大的方差，从而导致Softmax函数的梯度会比较小。因此，缩放点积模型可以较好地解决这个问题。双线性模型是一种泛化的点积模型．假设公式中 $𝑾 = 𝑼^T𝑽$，双线性模型可以写为 $𝑠(𝒙, 𝒒) = 𝒙^T𝑼^T𝑽𝒒 = (𝑼𝒙)^T(𝑽𝒒)$ ，即分别对 $𝒙$ 和$𝒒$ 进行线性变换后计算点积．相比点积模型，双线性模型在计算相似度时引入了非对称性。因此，点积缩放模型是目前使用最广泛的一种注意力形式。


注意力分布 $a_𝑛$ 可以解释为在给定任务相关的查询 $𝒒$ 时，第 $𝑛$ 个输入向量受关注的程度。这里采用一种“软性”的信息选择机制对输入信息进行汇总，即：

$attention(𝑿, 𝒒) =Σ_{n=1}^N a_n x_n=E_{z\sim p(z|X,q)}[x_z]$

上式称为软性注意力机制，其选择的信息是所有输入向量在注
意力分布下的期望，下图给出软性注意力机制的示例：

<div align=center>
<img src="./imgs/1.jpg" width="400" height="200">
</div>
<div align=center>图1.软性注意力机制</div>


此外，还有一种注意力是只关注某一个输入向量，称为硬性注意力。硬性注意力有两种实现方式：（1）一种是选取最高概率的一个输入向量，即 $att(𝑿, 𝒒) = 𝒙_{\tilde n}$，其中 ̂$\tilde 𝑛$ 为概率最大的输入向量的下标，即 ̂$\tilde 𝑛 = argmax_{n=1}^N a_n$。（2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息，使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算法进行训练。因此，硬性注意力通常需要使用强化学习来进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。


更一般地，可以用键值对格式来表示输入信息，其中“键”用来计算注意力分布 $a_𝑛$ ，“值”用来计算聚合信息。用 $(𝑲, 𝑽 ) = [(𝒌_1
, 𝒗_1), ⋯ , (𝒌_𝑁 , 𝒗_𝑁 )]$ 表示 $𝑁$ 组输入信息，给定任务相关的查询向量 $𝒒$ 时，注意力函数为 $attention((𝑲, 𝑽 ), 𝒒)=Σ_{n=1}^N a_n v_n=Σ_{n=1}^N(exp(s(k_n,q))/Σ_{j=1}^N exp(s(k_j,q)))v_n$

其中 $𝑠(𝒌_𝑛, 𝒒)$ 为打分函数，当 $𝑲 = 𝑽$ 时，键值对模式就等价于普通的注意力机制。


自注意力模型可以看作在一个线性投影空间中建立 $𝑯$ 中不同向量之间的交互关系。为了提取更多的交互信息，可以使用多头自注意力在多个不同的投影空间中捕捉不同的交互信息．假设在 $𝑀$ 个投影空间中分别应用自注意力模型。多头注意力是2017年《attention is all you need》提出的transformer架构所使用的注意力形式，利用多个查询 $𝑸 = [𝒒_1, ⋯ , 𝒒_𝑀]$ ，来并行地从输入信息中选取多组信息，每个注意力关注输入信息的不同部分：$MultiHead(𝑯) = 𝑾_𝑜[head_1; ⋯ ; head_𝑀]，head_𝑚 = self-att(𝑸_𝑚, 𝑲_𝑚, 𝑽_𝑚)，∀𝑚 ∈ {1, ⋯ , 𝑀},𝑸_m = 𝑾_𝑞𝑿 ∈ ℝ^{𝐷_𝑘×𝑁}$，$𝑲 = 𝑾_𝑘𝑿 ∈ ℝ^{𝐷_𝑘×N}$，$𝑽 = 𝑾_𝑣𝑿 ∈ ℝ^{𝐷_𝑣×N}，𝑾_o ∈ ℝ^{𝐷_h×Md_{v}}$为输出投影矩阵,$𝑾_𝑞^m ∈ ℝ^{𝐷_𝑘×𝐷_h} ,𝑾_𝑘^m ∈ ℝ^{𝐷_𝑘×𝐷_h} ,𝑾_𝑣^m ∈ ℝ^{𝐷_𝑣×𝐷_h}$为投影矩阵，$𝑚 ∈ {(1, ⋯ , 𝑀)}$。


基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系。虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互；另一种方法是使用全连接网络。全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列．不同的输入长度，其连接权重的大小也是不同的。这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型。为了提高模型能力，自注意力模型经常采用查询-键-值模式，其计算过程如下图所示，其中红色字母表示矩阵的维度。


<div align=center>
<img src="./imgs/2.jpg" width="400" height="200">
</div>
<div align=center>图2.软性注意力机制</div>


假设输入序列为 $𝑿 = [𝒙_1, ⋯ , 𝒙_𝑁]∈ℝ^{𝐷_𝑥×𝑁}$，输出序列为 $𝑯 = [𝒉_1, ⋯ , 𝒉_𝑁 ] ∈ℝ^{𝐷_𝑣×𝑁}$ ，自注意力模型的具体计算过程如下：

- 对于每个输入 $𝒙_𝑖$ ，首先将其线性映射到三个不同的空间，得到查询向量 $𝒒_𝑖 ∈ ℝ^{𝐷_𝑘}$ 、键向量 $𝒌_𝑖 ∈ ℝ^{𝐷_𝑘}$ 和值向量 $𝒗_𝑖 ∈ ℝ^{𝐷_v}$，对于整个输入序列 $𝑿$，线性映射过程可以简写为 $𝑸 = 𝑾_𝑞𝑿 ∈ ℝ^{𝐷_𝑘×𝑁}$，$𝑲 = 𝑾_𝑘𝑿 ∈ ℝ^{𝐷_𝑘×N}$，$𝑽 = 𝑾_𝑣𝑿 ∈ ℝ^{𝐷_𝑣×N}$。其中 $𝑾_𝑞 ∈ ℝ^{𝐷_𝑘×𝐷_𝑥} ,𝑾_𝑘 ∈ ℝ^{𝐷_𝑘×𝐷_𝑥} ,𝑾_𝑣 ∈ ℝ^{𝐷_𝑣×𝐷_𝑥}$ 分别为线性映射的参数矩阵，$𝑸 = [𝒒_1, ⋯ ,𝒒_𝑁 ], 𝑲 = [𝒌_1, ⋯ , 𝒌_𝑁 ], 𝑽 = [𝒗_1, ⋯ , 𝒗_𝑁]$ 分别是由查询向量、键向量和值向量构成的矩阵。
- 对于每一个查询向量 $𝒒_𝑛$ ∈ 𝑸，利用键值对注意力机制，可以得到输出向量 $𝒉_𝑛$ ，$𝒉_𝑛 = att((𝑲, 𝑽 ), 𝒒_𝑛)=Σ_{j=1}^N a_{nj}v_j=Σ_{j=1}^Nsoftmax(s(k_j,q_n))v_j$，其中 $𝑛, 𝑗 ∈ [1, 𝑁]$ 为输出和输入向量序列的位置，$a_{𝑛𝑗}$ 表示第 $𝑛$ 个输出关注到第 $𝑗$ 个输入的权重。


如果使用缩放点积来作为注意力打分函数，输出向量序列可以简写为：
$𝑯 = 𝑽softmax(𝑲^T𝑸/\sqrt𝐷_𝑘)$。


#### 5.1.2 seq2seq任务和encoder-decoder模型


在序列生成任务中，有一类任务是序列到序列(seq2seq)生成任务，即输入一个序列生成另一个序列。序列到序列是一种条件的序列生成问题，给定一个序列 $𝒙_{1∶𝑆}$ ，生成另一个序列 $𝒚_{1∶𝑇}$ 。输入序列的长度 $𝑆$ 和输出序列的长度 $𝑇$ 可以不同。实现序列到序列的最简单方法是使用两个循环神经网络来分别进行编码和解码，也称为编码器-解码器（Encoder-Decoder）模型。

- 编码器 首先使用一个循环神经网络 $𝑓_{enc}$ 来编码输入序列 $𝒙_{1∶𝑆}$ 得到一个固定维数的向量 $𝒖$，$𝒖$ 一般为编码循环神经网络最后时刻的隐状态。$h_t^{enc}=f_{enc}(h_{t-1}^{enc},e_{x_{t-1}},θ_{enc}), ∀𝑡 ∈ [1 ∶ 𝑆],𝒖 = 𝒉_S^{enc}$ ，其中 $𝑓_{enc}(⋅)$ 为编码循环神经网络，其参数为 $ θ_{enc} $，$𝒆_𝑥$ 为词 $𝑥$
的词向量。
- 解码器 在生成目标序列时，使用另外一个循环神经网络 $𝑓_dec$ 来进行解码．在解码过程的第 $𝑡$ 步时，已生成前缀序列为 $𝒚_{1∶(𝑡−1)}$。令 $𝒉_t^{dec}$ 表示在网络𝑓dec 的隐状态，$𝒐_𝑡 ∈ (0, 1)^{|𝒱|}$ 为词表中所有词的后验概率，则 $𝒉_0^{dec}=u,𝒉_t^{dec}= f_{dec} (h_{t-1}^{dec}, e_{𝑦_{𝑡−1}}, θ_{dec}),𝒐_𝑡=𝑔(𝒉_t^{dec}, θ_𝑜)$。


其中 $𝑓_{dec(⋅)}$ 为解码循环神经网络，$𝑔(⋅)$ 为最后一层为 Softmax 函数的前馈神经网络，$θ_{dec}$ 和 $θ_𝑜$ 为网络参数，$𝒆_𝑦$ 为 $𝑦$ 的词向量，$𝑦_0$ 为一个特殊符号，比如⟨𝐸𝑂𝑆⟩。基于循环神经网络的序列到序列模型的缺点是：
- 编码向量 $𝒖$ 的容量问题，输入序列的信息很难全部保存在一个固定维度的向量中；
- 当序列很长时，由于循环神经网络的长程依赖问题，容易丢失输入序列的信息。


为了获取更丰富的输入序列信息，可以在每一步中通过注意力机制来从输入序列中选取有用的信息。在解码过程的第 $𝑡$ 步时，先用上一步的隐状态 $𝒉^{dec}_{𝑡−1}$ 作为查询向量，利用注意力机制从所有输入序列的隐状态 $𝑯^{enc} = [𝒉^{enc}_1, ⋯ , 𝒉^{enc}_𝑆]$ 中选择相关信息:
$𝒄_𝑡 = att(𝑯^{enc}, 𝒉^{dec}_{𝑡−1}) =Σ_{𝑖=1}^S a_i h_i^{enc}=Σ_{i=1}^S softmax(s(h^{enc}_i,h^{dec}_{t-1}))h_i^{enc}$，其中 $𝑠(⋅)$ 为注意力打分函数。然后，将从输入序列中选择的信息 $𝒄_𝑡$ 也作为解码器 $𝑓_dec(⋅)$ 在第 $𝑡$ 步时的输入，得到第 $𝑡$ 步的隐状态:
$h^{dec}_t=f_{dec}(h^{dec}_{t-1},[e_{y_{t-1}};c_t],θ_{dec})$，最后，将 $𝒉^{dec}_𝑡$ 输入到分类器 $𝑔(⋅)$ 中来预测词表中每个词出现的概率。


#### 5.1.3 transformer


除长程依赖问题外，基于循环神经网络的序列到序列模型的另一个缺点是无法并行计算．为了提高并行计算效率以及捕捉长距离的依赖关系，我们可以使用自注意力模型来建立一个全连接的网络结构。本节介绍一个目前非常成功的基于自注意力的序列到序列模型：Transformer。


对于一个序列 $𝒙_{1∶𝑇}$，可以构建一个含有多层多头自注意力模块的模型来对其进行编码。由于自注意力模型忽略了序列 $𝒙_{1∶𝑇}$ 中每个 $𝒙_𝑡$ 的位置信息，因此需要在初始的输入序列中加入位置编码来进行修正。对于一个输入序列 $𝒙_{1∶𝑇} ∈ ℝ^{𝐷×𝑇}$，令 $𝑯(0) = [𝒆_{𝑥_1} + 𝒑_1, ⋯ , 𝒆_{𝑥_𝑇}+𝒑_𝑇]$，其中 $𝒆_{𝑥_𝑡}∈ ℝ^𝐷$ 为词 $𝑥_𝑡$ 的嵌入向量表示，$𝒑_𝑡 ∈ ℝ^𝐷$ 为位置 $𝑡$ 的向量表示，即位置编码。$𝒑_𝑡$ 可以作为可学习的参数,也可以通过下面方式进行预定义: $𝒑_{𝑡,2𝑖} = sin(𝑡/10000^{2𝑖/𝐷})，𝒑_{𝑡,2𝑖+1} = cos(𝑡/10000^{2𝑖/𝐷})$，其中 $𝒑_{𝑡,2𝑖}$ 表示第 $𝑡$个位置的编码向量的第 $2𝑖$ 维，$𝐷$ 是编码向量的维度。给定第 $𝑙−1$ 层的隐状态 $𝑯^(𝑙−1)$，第 $𝑙$ 层的隐状态 $𝑯^(𝑙)$ 可以通过一个多头自注意力模块和一个非线性的前馈网络得到．每次计算都需要残差连接以及层归一化操作。具体计算为：
$𝒁^{(𝑙)} =norm(𝑯^{(𝑙−1)} + MultiHead(𝑯^{(𝑙−1)}))，𝑯(𝑙) =norm(𝒁^{(𝑙)} + FFN(𝒁^{(𝑙)}))$


其中 $norm(⋅)$ 表示层归一化，$FFN(⋅)$ 表示逐位置的前馈神经网络，是一个简单的两层网络．对于输入序列中每个位置上向量 $𝒛 ∈ 𝒁^{(𝑙)}$，$FFN(𝒛) = 𝑾_2ReLu(𝑾_1𝒛 + 𝒃_1) + 𝒃_2，其中 $𝑾1,𝑾2,𝒃1,𝒃2$ 为网络参数。基于自注意力模型的序列编码可以看作一个全连接的前馈神经网络，第 $𝑙$ 层的每个位置都接受第 $𝑙−1$ 层的所有位置的输出．不同的是，其连接权重是通过注意力机制动态计算得到。


Transformer模型是一个基于多头自注意力的序列到序列模型，其整个网络结构可以分为两部分：
- 编码器只包含多层的多头自注意力模块，每一层都接受前一层的输出作为输入。编码器的输入为序列 $𝒙_{1∶𝑆}$，输出为一个向量序列 $𝑯^{enc} = [𝒉^{enc}_
1, ⋯ , 𝒉^{enc}_𝑆]$。然后，用两个矩阵将 $𝑯^{enc}$ 映射到 $𝑲^{enc}$ 和 $𝑽^{enc}$ 作为键值对供解码器使用，即 $𝑲^{enc} = 𝑾_𝑘^′𝑯^{enc}，V^{enc} = 𝑾_v^′𝑯^{enc}$。其中 $𝑾𝑘′$ 和 $𝑾𝑣′$ 为线性映射的参数矩阵。
- 解码器是通过自回归的方式来生成目标序列。和编码器不同，解码器由以下三个模块构成：(1)掩蔽自注意力模块：第 $𝑡$ 步时，先使用自注意力模型对已生成的前缀序列 $𝒚_{0∶(𝑡−1)}$ 进行编码得到 $𝑯^{dec} = [𝒉^{dec}_1, ⋯ , 𝒉^{dec}_𝑡]$；(2)解码器到编码器注意力模块：将 $𝒉^{dec}_𝑡$ 进行线性映射得到 $𝒒^{dec}_𝑡$。将 $𝒒^{dec}_𝑡$ 作为查询向量，通过键值对注意力机制来从输入 $(𝑲^{enc}, 𝑽^{enc})$ 中选取有用的信息；(3)逐位置的前馈神经网络：使用一个前馈神经网络来综合得到所有信息。


将上述三个步骤重复多次，最后通过一个全连接前馈神经网络来计算输出概率．下图给出了Transformer的网络结构示例，其中 $𝑁×$ 表示重复 $𝑁$ 次，“Add
& Norm” 表示残差连接和层归一化。在训练时，为了提高效率，通常将右移的目标序列 $𝒚_{0∶(𝑇−1)}$ 作为解码器的输入，即在第 $𝑡$ 个位置的输入为 $𝑦_{𝑡−1}$。在这种情况下，可以通过一个掩码来阻止每个位置选择其后面的输入信息。这种方式称为掩蔽自注意力。


<div align=center>
<img src="./imgs/3.jpg" width="600" height="600">
</div>
<div align=center>图3.transformer</div>


### 5.1.4 transformer代码解读


```python
if __name__ == '__main__':
    #1.数据集导入
    ## 句子的输入部分，
    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']
    #上方的列表是一个样本而不是三个样本，其中P（padding）,S（start）,E（end）分别代表编码端输入，解码端输入，解码端输入的真实标签（即正确答案）
    # E用来计算与解码端输出结果（训练预测答案）的损失，从而判断模型效果
    #P用来对输入矩阵进行填充，当batch-size大于1时，各样本的句子长度不一定相等，因此通过padding对矩阵的大小进行规范处理
    ## 构建词表，将字符与数字对应，更加方便计算机识别
    #编码端词表：
    src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}
    src_vocab_size = len(src_vocab)
    
    #解码端词表：
    tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}
    tgt_vocab_size = len(tgt_vocab)
    
    #规定编码端和解码端的输入句子长度，也就是输入矩阵的“列”
    src_len = 5 # length of source
    tgt_len = 5 # length of target
    ## 模型参数设置
    d_model = 512  # Embedding Size，字符转换为词向量的维度
    d_ff = 2048  # FeedForward dimension，前馈神经网络中线性层linear映射到的维度
    d_k = d_v = 64  # dimension of K(=Q), V
    n_layers = 6  # number of Encoder of Decoder Layer，6个Encoder和Decoder
    n_heads = 8  # number of heads in Multi-Head Attention，考虑多头注意力机制时，有8个头
## 1. 从整体网路结构来看，分为三个部分：编码层，解码层，输出层
class Transformer(nn.Module):
    def __init__(self):
        super(Transformer, self).__init__()
        self.encoder = Encoder()  ## 编码层
        self.decoder = Decoder()  ## 解码层
        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False) ## 输出层 ，d_model 是我们解码层每个token（预测结果）输出的维度大小，
        #之后会做一个 tgt_vocab_size 大小的softmax
        #把预测结果映射进解码端词表里，表现为各个词的概率大小
        
    ## forward接收输入 
    def forward(self, enc_inputs, dec_inputs):
        ## 这里有两个数据进行输入，一个是enc_inputs 形状为[batch_size, src_len]，主要是作为编码段的输入，一个dec_inputs，形状为[batch_size, tgt_len]，主要是作为解码端的输入
        
        #编码端的输出：
        ## enc_inputs作为输入 形状为[batch_size, src_len]。
        ## 输出格式由自己的函数内部指定，想要什么指定输出什么，可以是全部tokens的输出，可以是特定每一层的输出；也可以是中间某些参数的输出；
        ## enc_outputs就是主要的输出，enc_self_attns这里没记错的是QK转置相乘之后softmax之后的矩阵值，代表的是每个单词和其他单词相关性；
        enc_outputs, enc_self_attns = self.encoder(enc_inputs)
 
        # 解码端的输出：
        ## dec_outputs 是decoder主要输出，用于后续的linear映射； dec_self_attns类比于enc_self_attns 是查看每个单词对decoder中输入的其余单词的相关性；
        ## dec_enc_attns是decoder中每个单词对encoder中每个单词的相关性；
        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)
 
        ## 解码端输出结果到词表的映射
        ## dec_outputs做映射到词表大小
        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]
        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.src_emb = nn.Embedding(src_vocab_size, d_model)  ## 词向量层，这个其实就是去定义生成一个矩阵，大小是 src_vocab_size * d_model
        self.pos_emb = PositionalEncoding(d_model) ## 位置编码层，这部分自己实现，表示位置编码情况，这里是固定的正余弦函数，
        # 也可以使用类似词向量的nn.Embedding获得一个可以更新学习的位置编码
        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)]) ## 注意力机制和前馈神经网络，
        # 使用ModuleList对多个encoder进行堆叠，因为后续的encoder并没有使用词向量和位置编码，所以抽离出来；
 
    ## forward接收编码端输入
    def forward(self, enc_inputs):
        ## 这里我们的 enc_inputs（1个输入） 形状是： [batch_size x source_len]
 
        ## 下面这个代码通过src_emb，根据字符对应的数字进行索引定位，将数字对应的词向量提取出来，构成一个矩阵，enc_outputs输出形状是[batch_size, src_len, d_model]
        ## 总体思路是把输入词语由字符转换为数字，再由数字索引找到对应的词向量
        enc_outputs = self.src_emb(enc_inputs)
 
        ## 这里就是位置编码，把两者相加放入到了这个函数里面，从这里可以去看一下位置编码函数的实现；3.
        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)
 
        ## get_attn_pad_mask是为了得到句子中pad的位置信息，并得到一个相同大小的01矩阵，0表示词语，1表示填充的pad；
        ## 之所以要这样，是为了在计算自注意力和交互注意力的时候去掉pad符号的影响，否则会把pad视作句子中的词语研究；去看一下这个函数 4.
        ## 此处是编码端的自注意力机制，因此两个输入都是enc_inputs，在编码端和解码端之间的交互注意力机制中，两个输入不相同
        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)
        enc_self_attns = []
        
        ## 多个Encoder和Decoder，因此采用for循环进行
        for layer in self.layers:
            ## 去看EncoderLayer 层函数 5.
            ## 这里的输入分别是上一轮的输出结果，标识句子中pad位置的符号矩阵
            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)
            enc_self_attns.append(enc_self_attn)
        return enc_outputs, enc_self_attns
## 3. PositionalEncoding 代码实现，这部分的实现过程基本固定
class PositionalEncoding(nn.Module):
    ## max_len是句子的最大长度，结合padding对输入矩阵进行规范
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
 
        ## 位置编码的实现其实很简单，直接对照着公式去敲代码就可以，下面这个代码只是其中一种实现方式；
        ## 从理解来讲，需要注意的就是偶数和奇数在公式上有一个共同部分，我们使用log函数把次方拿下来，方便计算；
        ## pos代表的是单词在句子中的索引，这点需要注意；比如max_len是128个，那么索引就是从0，1，2，...,127
        ##假设我的d_model是512，2i那个符号中i从0取到了255，那么2i对应取值就是0,2,4...510
        self.dropout = nn.Dropout(p=dropout)
 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))## 表示公式中共有的部分，此处是将原公式进行了变化，利用了对数的运算，无误
        pe[:, 0::2] = torch.sin(position * div_term)## 公式中PE的求取；这里需要注意的是pe[:, 0::2]这个用法，就是从0开始到最后面，补长为2，其实代表的就是偶数位置
        pe[:, 1::2] = torch.cos(position * div_term)## 公式中PE的求取；这里需要注意的是pe[:, 1::2]这个用法，就是从1开始到最后面，补长为2，其实代表的就是奇数位置
        ## 上面代码获取之后得到的pe:[max_len*d_model]
 
        ## 下面这个代码之后，我们得到的pe形状是：[max_len*1*d_model]
        pe = pe.unsqueeze(0).transpose(0, 1)## transpose实现矩阵的转置，unsqueeze（0）是增加矩阵的维度，此处变为三维矩阵
        '''transpose（X，Y）函数和矩阵的转置是一个意思，相当于行为X轴，列为Y轴，X轴和Y轴调换了位置；
　　        X轴用0表示，Y轴用1表示；
　        　例如：如果transpose（1，0）表示行与列调换了位置；
            此处交换了x与y轴，否则pe的形状会是[1*max_len*d_model]'''
        
        '''一、先看torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。
            1.squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。
            2.a.squeeze(N) 就是去掉a中指定的维数为一的维度。
              还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的维数N为一的维度。
            二、再看torch.unsqueeze()这个函数主要是对数据维度进行扩充。
            给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度。
            还有一种形式就是b=torch.unsqueeze(a，N) a就是在a中指定位置N加上一个维数为1的维度'''
 
        self.register_buffer('pe', pe)  ## 定一个缓冲区，其实简单理解为这个参数不更新就可以
## 6. MultiHeadAttention
class MultiHeadAttention(nn.Module):
    def __init__(self):
        super(MultiHeadAttention, self).__init__()
        ## 输入进来的QKV是相等的，我们会使用映射linear做一个映射得到参数矩阵Wq, Wk,Wv
        self.W_Q = nn.Linear(d_model, d_k * n_heads)
        self.W_K = nn.Linear(d_model, d_k * n_heads)
        self.W_V = nn.Linear(d_model, d_v * n_heads)
        self.linear = nn.Linear(n_heads * d_v, d_model)
        self.layer_norm = nn.LayerNorm(d_model)
 
    def forward(self, Q, K, V, attn_mask):
 
        ## 这个多头分为这几个步骤，首先映射分头，然后计算atten_scores，然后计算atten_value;
        ##输入进来的数据形状： Q: [batch_size x len_q x d_model], K: [batch_size x len_k x d_model], V: [batch_size x len_k x d_model]
        residual, batch_size = Q, Q.size(0)
        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)
 
        ##下面这个就是先映射，后分头；一定要注意的是q和k分头之后维度是一致额，所以一看这里都是dk
        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]
        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]
        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]
 
        ## 输入进行的attn_mask形状是 batch_size x len_q x len_k，然后经过下面这个代码得到 新的attn_mask : [batch_size x n_heads x len_q x len_k]，就是把pad信息重复了n个头上
        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)
 
 
        ##然后我们计算 ScaledDotProductAttention 这个函数，去7.看一下
        ## 得到的结果有两个：context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q x len_k]
        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]
        output = self.linear(context)
        return self.layer_norm(output + residual), attn # output: [batch_size x len_q x d_model]
## 8. PoswiseFeedForwardNet
class PoswiseFeedForwardNet(nn.Module):
    def __init__(self):
        super(PoswiseFeedForwardNet, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.layer_norm = nn.LayerNorm(d_model)
 
    def forward(self, inputs):
        residual = inputs # inputs : [batch_size, len_q, d_model]
        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))
        output = self.conv2(output).transpose(1, 2)
        return self.layer_norm(output + residual)
```