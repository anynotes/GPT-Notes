第一章：预训练语言模型
-----------

> 涉及知识点：
> 
> *   单向语言模型、双向语言模型；
>     
> *   Transformer；
>     
> *   预训练任务，包括 MLM、NSP 等；
>     
> *   NLP 的任务类型以及 fine-tuning；
>     

预训练语言模型想必大家已经不再陌生，以 GPT、ELMO 和 BERT 为首的预训练语言模型在近两年内大放异彩。预训练语言模型主要分为单向和双向两种类型：

*   **单向** ：以 GPT 为首，强调 **从左向右** 的编码顺序，适用于 Encoder-Decoder 模式的自回归（Auto-regressive）模型；
    
*   **双向** ：以 ELMO 为首，强调从左向右和从右向左 **双向编码** ，但 ELMO 的主体是 LSTM，由于其是串形地进行编码，导致其运行速度较慢，因此最近 BERT 则以 Transformer 为主体结构作为双向语言模型的基准。

现如今常用的语言模型大多数是 BERT 及其变体，它的主体结构 Transformer 模型是由谷歌机器翻译团队在 17 年末提出的，是一种完全利用 attention 机制构建的端到端模型，具体算法详解可详情【预训练语言模型】Attention Is All You Need（Transformer）。

之所以选择 Transformer，是因为 **其完全以 Attention 作为计算推理技术** ，任意的两个 token 均可以两两交互，使得推理完全可以由矩阵乘机来替代，实现了 **可并行化计算** ，因此 Transformer 也可以认为是一个全连接图， **缓解了序列数据普遍存在的长距离依赖和梯度消失等缺陷** 。

> 在 NLP 领域中，Attention 机制的目标是对具有强相关的 token 之间提高模型的关注度。例如在文本分类中，部分词对分类产生的贡献更大，则会分配较大的权重。
> 
> 对句子的编码主要目标是为了让模型记住 token 的语义。传统的 LSTM 则只能通过长短期记忆的方法来捕捉 token 之间的关系，容易导致梯度消失或记忆模糊问题，而 Transformer 中，任意的 token 之间都有显式的连接，避免了长距离依赖性问题。当然 Transformer 也增加了 position embedding 以区分不同 token 的位置关系，