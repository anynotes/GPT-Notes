# 《GPT笔记 GPT-Notes》

![封面](./imgs/1.jpg)

### 重点

**由于作者水平有限，希望大家积极提交改进意见**, 您可按[**文章撰写规范**](./文章撰写规范.md)对相关内容进行修改或新增。**我们一起对内容进行整理、完善，确保文档的准确性**。

## 一、引言

GPT（Generative Pre-trained Transformer）是一种基于Transformer架构的自然语言处理模型。它通过预训练和微调的方式，可以完成各种自然语言处理任务，如文本生成、问答、摘要等。本介绍将概述GPT的基本原理、架构和应用。

## 二、基本原理

GPT是一种基于Transformer架构的自回归模型，它通过预测下一个单词来生成文本。GPT使用无监督的预训练方法，通过大量的文本数据进行训练，学习到丰富的语言知识。然后，通过有监督的微调方法，将预训练好的模型应用于具体的自然语言处理任务。

## 三、架构

GPT的架构主要由以下几个部分组成：

* Transformer：GPT的核心部分，用于处理输入的文本序列。Transformer由多层自注意力机制和前馈神经网络组成，可以捕捉文本中的长距离依赖关系。
* 位置编码：为了处理可变长度的文本序列，GPT使用位置编码来表示输入序列中每个单词的位置信息。
* 预训练任务：GPT通过预测下一个单词、文本分类、命名实体识别等任务进行预训练，学习到丰富的语言知识。
* 微调任务：GPT通过有监督的微调方法，将预训练好的模型应用于具体的自然语言处理任务，如文本生成、问答、摘要等。
  
## 四、目录